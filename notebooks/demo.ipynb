{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4032d086-3eaf-48d3-8adf-5c350afc06d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4-32k'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "parent_directory = os.path.abspath('..')\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "\n",
    "from actionweaver.llms import patch\n",
    "from langchain_community.utilities.github import GitHubAPIWrapper\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "from autocoder.bot import AutoCoder\n",
    "from autocoder.index import RepositoryIndex\n",
    "\n",
    "from autocoder.codebase import Codebase\n",
    "\n",
    "os.environ[\"MODEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd3dc08-68d8-49a9-95c4-f1d5a1c178d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_repository = \"TengHu/AutoCoder\"\n",
    "github_api = GitHubAPIWrapper(\n",
    "    github_repository=github_repository,\n",
    "    github_app_id=os.environ[\"GITHUB_APP_ID\"],\n",
    "    github_app_private_key=os.environ[\"GITHUB_APP_PRIVATE_KEY\"],\n",
    ")\n",
    "codebase = Codebase(github_api)\n",
    "index = RepositoryIndex(github_repository, codebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd099969-a66c-4054-8015-25b46fdbb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_coder = AutoCoder(index, codebase, create_branch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c39ea4-5a13-4edf-9942-8e8474367b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = '''[Code Change] Enhance the 'Example Pull Requests' section in the README.md to be more detailed and descriptive.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e83542-1cb0-415a-9bde-94f04155bb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[DEBUG]\u001b[0mPlanning code change: Enhance the 'Example Pull Requests' section in the README.md to be more detailed and descriptive.\n",
      "\u001b[92m[DEBUG]\u001b[0mImplementation plan created: Modify the README.md file to add more details to the 'Example Pull Requests' section.\n",
      "\u001b[96mREADME.md\u001b[0m:In the 'Example Pull Requests' section, add more details about what each pull request achieved, what changes it introduced to the codebase, why it was necessary, and any challenges faced or lessons learned during its development.\n",
      "\n",
      "\u001b[92m[DEBUG]\u001b[0mModifying file: README.md\n",
      "\u001b[92m[DEBUG]\u001b[0mCommitting code change to README.md\n",
      "\u001b[31m- 1. Instruction: Enhance the 'Example Pull Requests' section in the README.md to be more detailed and descriptive. ([PR](https://github.com/TengHu/AutoCoder/pull/77), [LangSmith traces](https://smith.langchain.com/public/11db987a-70c6-4f96-97b6-77db702e67d0/r))\u001b[0m\n",
      "\u001b[32m+ 1. Instruction: Enhance the 'Example Pull Requests' section in the README.md to be more detailed and descriptive. This pull request was necessary to improve the clarity and usefulness of the documentation. It involved careful editing and proofreading to ensure accuracy and readability. ([PR](https://github.com/TengHu/AutoCoder/pull/77), [LangSmith traces](https://smith.langchain.com/public/11db987a-70c6-4f96-97b6-77db702e67d0/r))\u001b[0m\n",
      "  \n",
      "\u001b[31m- 2. Instruction: Updating `autocoder/bot.py` to use AzureOpenAI instead of OpenAI client. ([PR](https://github.com/TengHu/AutoCoder/pull/81), [LangSmith traces](https://smith.langchain.com/o/2a666482-a835-4718-9413-7991c7a8fbdf/projects/p/080603e5-0dba-4e2b-ab57-bd6707f355f2?timeModel=%7B%22duration%22%3A%227d%22%7D&tab=0&runtab=0&peek=1c3f8247-bc5f-4626-93c9-a1e7b776006c))\u001b[0m\n",
      "\u001b[32m+ 2. Instruction: Updating `autocoder/bot.py` to use AzureOpenAI instead of OpenAI client. This change was required to take advantage of the superior capabilities of AzureOpenAI. The main challenge was ensuring compatibility with the existing codebase. ([PR](https://github.com/TengHu/AutoCoder/pull/81), [LangSmith traces](https://smith.langchain.com/o/2a666482-a835-4718-9413-7991c7a8fbdf/projects/p/080603e5-0dba-4e2b-ab57-bd6707f355f2?timeModel=%7B%22duration%22%3A%227d%22%7D&tab=0&runtab=0&peek=1c3f8247-bc5f-4626-93c9-a1e7b776006c))\u001b[0m\n",
      "\u001b[32m+ \u001b[0m\n",
      "\u001b[32m+ 3. Instruction: Move all classes from autocoder/pydantic_models/file_ops.py into separate files, one for each class. Remove the code in original file. This refactoring was necessary to improve the organization and readability of the code. It was a straightforward task, but required careful attention to avoid introducing bugs. ([PR](https://github.com/TengHu/AutoCoder/pull/85),  [LangSmith traces](https://smith.langchain.com/o/2a666482-a835-4718-9413-7991c7a8fbdf/projects/p/080603e5-0dba-4e2b-ab57-bd6707f355f2?timeModel=%7B%22duration%22%3A%227d%22%7D&tab=0&runtab=0&peek=fecc131a-0a95-49a2-9ec3-b8bc503efe40))\u001b[0m\n",
      "\u001b[32m+ \u001b[0m\n",
      "\u001b[32m+ 4. Instruction: Create a new file named autocoder/test_codebase.py. In this file, write unit tests for Codebase class using pytest.  For each method in the Codebase class, there should be a corresponding test method in test_codebase.py, and within each test method, the details of the test case should be implemented. This was a crucial task to ensure the reliability and correctness of the Codebase class. It required a good understanding of the class's functionality and the principles of unit testing.\u001b[0m\n",
      "\u001b[31m- 3. Instruction: \u001b[0m\n",
      "\u001b[31m- Move all classes from autocoder/pydantic_models/file_ops.py into separate files, one for each class. Remove the code in original file.\u001b[0m\n",
      "\u001b[31m- ([PR](https://github.com/TengHu/AutoCoder/pull/85),  [LangSmith traces](https://smith.langchain.com/o/2a666482-a835-4718-9413-7991c7a8fbdf/projects/p/080603e5-0dba-4e2b-ab57-bd6707f355f2?timeModel=%7B%22duration%22%3A%227d%22%7D&tab=0&runtab=0&peek=fecc131a-0a95-49a2-9ec3-b8bc503efe40))\u001b[0m\n",
      "\u001b[31m- 4. Instruction: \u001b[0m\n",
      "\u001b[31m- Create a new file named autocoder/test_codebase.py. In this file, write unit tests for Codebase class using pytest.  For each method in the Codebase class, there should be a corresponding test method in test_codebase.py, and within each test method, the details of the test case should be implemented.\u001b[0m\n",
      "The 'Example Pull Requests' section in the README.md file has been updated to be more detailed and descriptive.\n"
     ]
    }
   ],
   "source": [
    "res = auto_coder(input)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f2004ec-842d-4886-b319-941aded910ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Pull Request has been successfully created with the PR number 90. It includes the new file 'autocoder.sch' which contains the rewritten AutoCoder class in Scheme. The changes were made in the aw_demo_bot_v6 branch.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_coder(\"create PR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b223a-e692-4293-8598-56ad04009f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c26fa26-9f69-4921-b605-c73f810d612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m [DEBUG] Gathering context: df \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_prompt = \"df\"\n",
    "print(\"\\033[1m\", f\"[DEBUG] Gathering context: {user_prompt}\", \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45241dc8-6d3a-407c-b70f-336ec148780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      def list_files_in_bot_branch(self):\n",
      "\u001b[32m+         print('Listing files in bot branch...')\u001b[0m\n",
      "          content = self.github_api.list_files_in_bot_branch()\n",
      "\u001b[31m-         files = content.split(\"\\n\")[1:]\u001b[0m\n",
      "\u001b[32m+         files = content.split('\\n')[1:]\u001b[0m\n",
      "\u001b[32m+         print(f'Found {len(files)} files.')\u001b[0m\n",
      "\u001b[32m+         sdfsf\u001b[0m\n",
      "\u001b[32m+         f\u001b[0m\n",
      "\u001b[32m+         f\u001b[0m\n",
      "\u001b[32m+         f\u001b[0m\n",
      "\u001b[32m+         f\u001b[0m\n",
      "          return files\n"
     ]
    }
   ],
   "source": [
    "def get_diff(old_text: str, new_text: str) -> str:\n",
    "    # Split the text into lines\n",
    "    old_lines = old_text.splitlines()\n",
    "    new_lines = new_text.splitlines()\n",
    "    \n",
    "    # Create a differ object\n",
    "    differ = difflib.Differ()\n",
    "    \n",
    "    # Compare the lines\n",
    "    diff = list(differ.compare(old_lines, new_lines))\n",
    "\n",
    "    res = []\n",
    "    for line in diff:\n",
    "        if line.startswith('+'):\n",
    "            res.append(\"\\033[32m\" + line + \"\\033[0m\")  # Green color for additions\n",
    "        elif line.startswith('-'):\n",
    "            res.append(\"\\033[31m\" + line + \"\\033[0m\")  # Red color for deletions\n",
    "        elif line.startswith('?'):\n",
    "            continue \n",
    "        else:\n",
    "            res.append(line)\n",
    "\n",
    "    return '\\n'.join(res)\n",
    "\n",
    "# Define the old and new text\n",
    "old_text = \"\"\"    def list_files_in_bot_branch(self):\n",
    "        content = self.github_api.list_files_in_bot_branch()\n",
    "        files = content.split(\"\\\\n\")[1:]\n",
    "        return files\"\"\"\n",
    "\n",
    "new_text = \"\"\"    def list_files_in_bot_branch(self):\n",
    "        print('Listing files in bot branch...')\n",
    "        content = self.github_api.list_files_in_bot_branch()\n",
    "        files = content.split('\\\\n')[1:]\n",
    "        print(f'Found {len(files)} files.')\n",
    "        sdfsf\n",
    "        f\n",
    "        f\n",
    "        f\n",
    "        f\n",
    "        return files\"\"\"\n",
    "\n",
    "\n",
    "print (get_diff(old_text, new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2571194-d2fa-491d-a60c-529a444a2a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      def list_files_in_bot_branch(self):\n",
      "+         print('Listing files in bot branch...')\n",
      "          content = self.github_api.list_files_in_bot_branch()\n",
      "-         files = content.split(\"\\n\")[1:]\n",
      "?                               ^  ^\n",
      "\n",
      "+         files = content.split('\\n')[1:]\n",
      "?                               ^  ^\n",
      "\n",
      "+         print(f'Found {len(files)} files.')\n",
      "          return files\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "# Define the old and new text\n",
    "old_text = \"\"\"    def list_files_in_bot_branch(self):\n",
    "        content = self.github_api.list_files_in_bot_branch()\n",
    "        files = content.split(\"\\\\n\")[1:]\n",
    "        return files\"\"\"\n",
    "\n",
    "new_text = \"\"\"    def list_files_in_bot_branch(self):\n",
    "        print('Listing files in bot branch...')\n",
    "        content = self.github_api.list_files_in_bot_branch()\n",
    "        files = content.split('\\\\n')[1:]\n",
    "        print(f'Found {len(files)} files.')\n",
    "        return files\"\"\"\n",
    "\n",
    "# Split the text into lines\n",
    "old_lines = old_text.splitlines()\n",
    "new_lines = new_text.splitlines()\n",
    "\n",
    "# Create a differ object\n",
    "differ = difflib.Differ()\n",
    "\n",
    "# Compare the lines\n",
    "diff = list(differ.compare(old_lines, new_lines))\n",
    "\n",
    "# Join the lines with line breaks and print\n",
    "print('\\n'.join(diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67af88f9-99c3-4b86-a11b-5eb6c3b76bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    '      def list_files_in_bot_branch(self):',\n",
    "    \"+         print('Listing files in bot branch...')\",\n",
    "    '          content = self.github_api.list_files_in_bot_branch()',\n",
    "    '-         files = content.split(\"\\\\n\")[1:]',\n",
    "    '?                               ^  ^',\n",
    "    \"+         files = content.split('\\\\n')[1:]\",\n",
    "    '?                               ^  ^',\n",
    "    \"+         print(f'Found {len(files)} files.')\",\n",
    "    '          return files'\n",
    "]\n",
    "\n",
    "\n",
    "res = []\n",
    "for line in lines:\n",
    "    if line.startswith('+'):\n",
    "        res.append(\"\\033[32m\" + line + \"\\033[0m\")  # Green color for additions\n",
    "    elif line.startswith('-'):\n",
    "        res.append(\"\\033[31m\" + line + \"\\033[0m\")  # Red color for deletions\n",
    "    elif line.startswith('?'):\n",
    "        continue \n",
    "    else:\n",
    "        res.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1df00b3-0aed-4881-87ca-09f99462e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      def list_files_in_bot_branch(self):\n",
      "\u001b[32m+         print('Listing files in bot branch...')\u001b[0m\n",
      "          content = self.github_api.list_files_in_bot_branch()\n",
      "\u001b[31m-         files = content.split(\"\\n\")[1:]\u001b[0m\n",
      "\u001b[32m+         files = content.split('\\n')[1:]\u001b[0m\n",
      "\u001b[32m+         print(f'Found {len(files)} files.')\u001b[0m\n",
      "          return files\n"
     ]
    }
   ],
   "source": [
    "print ('\\n'.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a7c88-6e37-4c06-a277-6063e78ba496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdb8a3-e2fe-4f3a-8ae5-b76b310cc46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68800d75-5151-4a49-aa16-07ce0a85a88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cf836-0365-4739-9f9c-a3ed111a36ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ece51c14-454a-4012-bbb7-af95275cbea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4-1106-preview'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocoder.pydantic_models.code_block_ops import create_blocks\n",
    "os.environ[\"MODEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0f73954-d533-4ec9-80dc-c0d3e764b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = '''<old_code_block>\n",
    "    def __init__(self, github_api):\n",
    "        self.github_api = github_api\n",
    "        self.file2code = {}\n",
    "</old_code_block>\n",
    "<instruction_to_rewrite_the_code_block>\n",
    "Add a print statement to log when the Codebase class is instantiated.\n",
    "</instruction_to_rewrite_the_code_block>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c0ec142-b796-43cb-a827-cd5fae889203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocoder.pydantic_models.code_block_ops import create_code\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Create code response in JSON. Add the leading whitespace to the new code block the same as the old code block.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            input\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "# response_format\n",
    "# temperature\n",
    "# presence_penalty\n",
    "\n",
    "code = create_code.invoke(\n",
    "    auto_coder.client,\n",
    "    messages=messages,\n",
    "    response_format = { \"type\": \"json_object\" },\n",
    "    model='gpt-4-1106-preview',\n",
    "    stream=False,\n",
    "    temperature = 0.1,\n",
    "    presence_penalty = 2.0,\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f1bd44-7a83-413c-aa27-91a28a36c9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def __init__(self, github_api):\n",
      "    print('Codebase class instantiated with GitHub API')\n",
      "    self.github_api = github_api\n",
      "    self.file2code = {}\n"
     ]
    }
   ],
   "source": [
    "print (\"def __init__(self, github_api):\\n    print('Codebase class instantiated with GitHub API')\\n    self.github_api = github_api\\n    self.file2code = {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5d0c1-b140-43fc-8360-dd11ab2aa3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e038327-cc51-47d9-b698-e0b37bb77ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788b734-70b4-4f15-ade4-8c6cdb37bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_blocks(operations):\n",
    "    for o in operations:\n",
    "        print ('#' * 10)\n",
    "        # print(f\"\\nfirst_line_of_original_block:\\n{o.first_line_of_original_block}\")\n",
    "        # print(f\"\\nlast_line_of_original_block:\\n{o.last_line_of_original_block}\")\n",
    "        print(f\"\\nnew_code:\\n{o.new_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b09137a8-1e71-4ecc-905f-74d31ccbc06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "\n",
      "new_code:\n",
      "        # self.client = trace_client(OpenAI())\n",
      "        self.client = trace_client(AzureOpenAI(\n",
      "            azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
      "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
      "            api_version=\"2023-10-01-preview\"\n",
      "        ))\n"
     ]
    }
   ],
   "source": [
    "print_blocks(blocks[0].operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7a650a5-d271-4ef5-89e9-ca0c33c7c794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c14d91e6-92c8-4bd5-8c01-1c299c52481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        self.client = trace_client(AzureOpenAI(\n",
      "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
      "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
      "            api_version=\"2023-10-01-preview\"\n",
      "        ))\n"
     ]
    }
   ],
   "source": [
    "print (\"        self.client = trace_client(AzureOpenAI(\\n            azure_endpoint=os.getenv(\\\"AZURE_OPENAI_ENDPOINT\\\"),\\n            api_key=os.getenv(\\\"AZURE_OPENAI_KEY\\\"),\\n            api_version=\\\"2023-10-01-preview\\\"\\n        ))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac9d72b-ce40-464a-9bfe-52bc3498bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"Create a PR\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5140a5d7-c0d7-44fe-9cb4-1fae8e58a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocoder.pydantic_models.context import create_context, gather_context\n",
    "res = gather_context(\"unused imports autocoder/bot\", auto_coder.client, index, codebase, add_line_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21801f8-c897-4270-b8e3-c4abb83d04e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c50e6dc6-e561-411a-b496-b4bc79f64ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocoder.pydantic_models.code_block_ops import BlockOpOnLineIdx\n",
    "\n",
    "block = BlockOpOnLineIdx(\n",
    "        start_line_code = \"\",\n",
    "        start_line_idx=19, \n",
    "        end_line_idx=23, \n",
    "        old_code=\"// self.client = trace_client(AzureOpenAI(\\n//     azure_endpoint = os.getenv(\\\"AZURE_OPENAI_ENDPOINT\\\"),\\n//     api_key=os.getenv(\\\"AZURE_OPENAI_KEY\\\"),\\n//     api_version=\\\"2023-10-01-preview\\\"\\n// ))\\nself.client = trace_client(OpenAI())\",\n",
    "        new_code=\"self.client = trace_client(AzureOpenAI(\\n    azure_endpoint = os.getenv(\\\"AZURE_OPENAI_ENDPOINT\\\"),\\n    api_key=os.getenv(\\\"AZURE_OPENAI_KEY\\\"),\\n    api_version=\\\"latest version\\\"\\n))\\n// self.client = trace_client(OpenAI())\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2084f6a-09ed-47eb-a4f5-c89429ec7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = github_api.read_file(\"autocoder/bot.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c559a335-adb2-4b44-9f6d-d5e1876fc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_code = block.find_block(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af2df60e-4607-480e-a431-80858d257fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        # self.client = trace_client(AzureOpenAI(\n",
      "        #     azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
      "        #     api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
      "        #     api_version=\"2023-10-01-preview\"\n",
      "        # ))\n"
     ]
    }
   ],
   "source": [
    "print (old_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1560df58-0448-4f42-8895-7448e1b0876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.client = trace_client(AzureOpenAI(\n",
      "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
      "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
      "    api_version=\"latest version\"\n",
      "))\n",
      "// self.client = trace_client(OpenAI())\n"
     ]
    }
   ],
   "source": [
    "print (block.new_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5faefd-139b-4938-846c-95b723876e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.client = trace_client(AzureOpenAI(\n",
      "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
      "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
      "    api_version=\"2023-10-01-preview\"\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "print (\"self.client = trace_client(AzureOpenAI(\\n    azure_endpoint = os.getenv(\\\"AZURE_OPENAI_ENDPOINT\\\"),\\n    api_key=os.getenv(\\\"AZURE_OPENAI_KEY\\\"),\\n    api_version=\\\"2023-10-01-preview\\\"\\n))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f81df0b8-0e82-4cfb-8482-44a00e68658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] Branch 'aw_demo_bot_v21' created successfully, and set as current active branch.\n"
     ]
    }
   ],
   "source": [
    "auto_coder = AutoCoder(github_api, index, codebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c7d2032-f8cc-46da-b39d-34be84833485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "old = \"\"\"# self.client = trace_client(AzureOpenAI(\n",
    "#     azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "#     api_version=\"2023-10-01-preview\"\n",
    "# ))\"\"\"\n",
    "new = \"self.client = trace_client(AzureOpenAI(\\n    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),\\n    api_key=os.getenv('AZURE_OPENAI_KEY'),\\n    api_version='2023-10-01-preview'))\"\n",
    "file_path = \"autocoder/bot.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a46756f-492c-4636-9e99-8f11a36e5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = f\"\"\"{file_path}\\nOLD <<<<\\n{old}\\n>>>> OLD\\nNEW <<<<\\n{new}\\n >>>> NEW\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f124a321-a535-49c6-bf89-3fd383e9533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/tenghu/Code/auto_coder/.venv/lib/python3.9/site-packages/langchain_community/utilities/github.py\u001b[0m(677)\u001b[0;36mupdate_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    675 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    676 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 677 \u001b[0;31m            \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    678 \u001b[0;31m            updated_file_content = file_content.replace(\n",
      "\u001b[0m\u001b[0;32m    679 \u001b[0;31m                \u001b[0mold_file_contents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_file_contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  print (file_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocoder/bot.py\n",
      "OLD <<<<\n",
      "# self.client = trace_client(AzureOpenAI(\n",
      "#     azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
      "#     api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
      "#     api_version=\"2023-10-01-preview\"\n",
      "# ))\n",
      ">>>> OLD\n",
      "NEW <<<<\n",
      "self.client = trace_client(AzureOpenAI(\n",
      "    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
      "    api_key=os.getenv('AZURE_OPENAI_KEY'),\n",
      "    api_version='2023-10-01-preview'))\n",
      " >>>> NEW\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'File content was not updated because old content was not found.It may be helpful to use the read_file action to get the current file contents.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_api.update_file(content)\n",
    "#file_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41621da-8647-49b9-987c-5539bd212eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137b4db-2208-4faf-a479-17076da18e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f31e7-e9a2-49a3-96b4-a182706388a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\n",
    "create a new file for each problem below with solution in lisp\n",
    "save these files under <proble_name>.lisp, e.g. problem1.lisp\n",
    "\n",
    "198. House Robber\n",
    "1. Two Sum\n",
    "2. Add Two Numbers\n",
    "3. Longest Substring Without Repeating Characters\n",
    "4. Median of Two Sorted Arrays\n",
    "\"\"\"\n",
    "\n",
    "input = \"\"\"\n",
    "find and remove unused imports in file autocoder/bot.\n",
    "\"\"\"\n",
    "\n",
    "input = \"\"\"[FILE UPDATE] Move the implementation of trace_client function into file autocoder/bot.py as a global function\"\"\"\n",
    "input = \"\"\"Refactor the RepositoryIndex class\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6fbc4-e66a-408e-867d-b617b6703275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73751d5b-cfaa-4885-8445-746a551da29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have successfully created a pull request with the number 73.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_coder(\"create a pr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d16a64d-de27-430f-b742-7c045af08813",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = index.query(\"AutoCoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d3d0e-232b-4f61-98a1-7eab421d7e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26a71e4f-054c-46d7-8a43-66b07fed9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion_message import ChatCompletionMessage\n",
    "from openai.types.chat.chat_completion_message_tool_call import (\n",
    "    ChatCompletionMessageToolCall,\n",
    "Function\n",
    ")\n",
    "messages = [{'role': 'system',\n",
    "  'content': 'You are a coding assistant, you have the capability to assist with code-related tasks and modify files.'},\n",
    " {'role': 'user', 'content': 'Move trace_client function to bot.py.'},\n",
    " ChatCompletionMessage(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(id='call_9f2agXdE5nnUl1X04XkYHjeQ', function=Function(arguments='{\"description\":\"Move the \\'trace_client\\' function from its current location to the \\'bot.py\\' file.\"}', name='PlanCodeChange'), type='function')]),\n",
    " {'tool_call_id': 'call_9f2agXdE5nnUl1X04XkYHjeQ',\n",
    "  'role': 'tool',\n",
    "  'name': 'PlanCodeChange',\n",
    "  'content': \"- New files created: []\\n        - Existing files updated: [['Updated file autocoder/bot.py', 'Updated file autocoder/telemetry.py']]\\n        - Problems encountered: []\\n        ###\\n    Ignore others,    Make the previous message more user-friendly\\n        \"}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1db6eea-8727-4b79-b971-217875e64fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e24e29-2df1-46c3-9fed-e60eeb76e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = '''<context>:\n",
    "<snippet autocoder/bot.py start_line=0 end_line=215>\n",
    "0. import os\n",
    "1. from typing import List\n",
    "2. \n",
    "3. from actionweaver import action\n",
    "4. from actionweaver.utils.tokens import TokenUsageTracker\n",
    "5. from openai import OpenAI\n",
    "6. \n",
    "7. from autocoder.pydantic_models.context import create_context, gather_context\n",
    "8. from autocoder.pydantic_models.file_ops import create_implementation_plan\n",
    "9. from autocoder.telemetry import trace_client, traceable\n",
    "10. \n",
    "11. assert os.environ[\"MODEL\"]\n",
    "12. MODEL = os.environ[\"MODEL\"]\n",
    "13. \n",
    "14. \n",
    "15. class AutoCoder:\n",
    "16.     def __init__(self, index, codebase, create_branch=True):\n",
    "17.         # self.client = trace_client(AzureOpenAI(\n",
    "18.         #     azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "19.         #     api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "20.         #     api_version=\"2023-10-01-preview\"\n",
    "21.         # ))\n",
    "22.         self.client = trace_client(OpenAI())\n",
    "23.         self.system_message = {\n",
    "24.             \"role\": \"system\",\n",
    "25.             \"content\": \"You are a coding assistant, you have the capability to assist with code-related tasks and modify files.\",\n",
    "26.         }\n",
    "27.         self.messages = [self.system_message]\n",
    "28.         self.index = index\n",
    "29.         self.codebase = codebase\n",
    "30. \n",
    "31.         if create_branch:\n",
    "32.             msg = self.create_branch(f\"aw_demo_bot\")\n",
    "33.             print(f\"[System] {msg}\")\n",
    "34. \n",
    "35.     def __call__(self, input: str):\n",
    "36.         self.original_input = input\n",
    "37. \n",
    "38.         self.messages.append(\n",
    "39.             {\n",
    "40.                 \"role\": \"user\",\n",
    "41.                 \"content\": \"<user_query>\\n\" + input + \"\\n</user_query>\",\n",
    "42.             }\n",
    "43.         )\n",
    "44. \n",
    "45.         response = self.client.chat.completions.create(\n",
    "46.             model=MODEL,\n",
    "47.             messages=self.messages,\n",
    "48.             stream=False,\n",
    "49.             temperature=1.0,\n",
    "50.             actions=[\n",
    "51.                 self.get_issues,\n",
    "52.                 self.question_answer,\n",
    "53.                 self.create_pull_request,\n",
    "54.                 self.plan_code_change,\n",
    "55.             ],\n",
    "56.             orch={\n",
    "57.                 self.plan_code_change.name: self.summarize_changes,\n",
    "58.                 self.create_pull_request.name: None,\n",
    "59.             },\n",
    "60.             token_usage_tracker=TokenUsageTracker(500),\n",
    "61.         )\n",
    "62. \n",
    "63.         content = \"\"\n",
    "64.         try:\n",
    "65.             content = response.choices[0].message.content\n",
    "66.         except:\n",
    "67.             content = str(response)\n",
    "68.         self.messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "69.         return content\n",
    "70. \n",
    "71.     @action(name=\"QuestionAnswer\", decorators=[traceable(run_type=\"tool\")])\n",
    "72.     def question_answer(self, rewritten_query: str, keywords: List[str]):\n",
    "73.         \"\"\"Answer questions about the codebase\"\"\"\n",
    "74. \n",
    "75.         context = gather_context(\n",
    "76.             \" \".join(keywords), self.client, self.index, self.codebase\n",
    "77.         )\n",
    "78. \n",
    "79.         messages = [\n",
    "80.             {\n",
    "81.                 \"role\": \"user\",\n",
    "82.                 \"content\": f\"{context} \\n###########\\n Question: {rewritten_query}\",\n",
    "83.             }\n",
    "84.         ]\n",
    "85.         response = self.client.chat.completions.create(\n",
    "86.             model=MODEL,\n",
    "87.             messages=messages,\n",
    "88.             stream=False,\n",
    "89.             token_usage_tracker=TokenUsageTracker(500),\n",
    "90.         )\n",
    "91.         return response\n",
    "92. \n",
    "93.     @action(name=\"GetIssues\", decorators=[traceable(run_type=\"tool\")])\n",
    "94.     def get_issues(self):\n",
    "95.         \"\"\"\n",
    "96.         Get a list of issues from the GitHub repo.\n",
    "97.         \"\"\"\n",
    "98.         response = self.codebase.get_issues()\n",
    "99.         response = response.split(\"\\n\")\n",
    "100.         return eval(response[1]) if len(response) > 1 else []\n",
    "101. \n",
    "102.     @action(name=\"CreateGitBranch\", decorators=[traceable(run_type=\"tool\")])\n",
    "103.     def create_branch(self, branch: str):\n",
    "104.         \"\"\"\n",
    "105.         Create a new Git branch.\n",
    "106.         \"\"\"\n",
    "107.         return self.codebase.create_branch(branch)\n",
    "108. \n",
    "109.     @action(name=\"CreatePullRequest\", decorators=[traceable(run_type=\"tool\")])\n",
    "110.     def create_pull_request(self, title: str, description: str):\n",
    "111.         \"\"\"\n",
    "112.         Create a new Pull Request in a Git repository.\n",
    "113. \n",
    "114.         Args:\n",
    "115.             title (str): The title of the Pull Request.\n",
    "116.             description (str): The description of the Pull Request.\n",
    "117.         \"\"\"\n",
    "118.         return (\n",
    "119.             f\"Current branch {self.codebase.get_active_branch()}\\n\"\n",
    "120.             + self.codebase.create_pull_request(pr_query=f\"{title}\\n {description}\")\n",
    "121.         )\n",
    "122. \n",
    "123.     @action(\n",
    "124.         \"PlanAndImplementCodeChange\",\n",
    "125.         stop=False,\n",
    "126.         decorators=[traceable(run_type=\"tool\")],\n",
    "127.     )\n",
    "128.     def plan_code_change(self, input: str):\n",
    "129.         \"\"\"\n",
    "130.         Plan and implement code changes based on a given description.\n",
    "131.         \"\"\"\n",
    "132. \n",
    "133.         context = gather_context(input, self.client, self.index, self.codebase)\n",
    "134. \n",
    "135.         files = [\n",
    "136.             file for file in self.codebase.list_files_in_bot_branch() if \".py\" in file\n",
    "137.         ]\n",
    "138. \n",
    "139.         \"\"\" \n",
    "140.         <files>\n",
    "141.         \n",
    "142.         </files>\n",
    "143.         \"\"\"\n",
    "144. \n",
    "145.         messages = [\n",
    "146.             {\n",
    "147.                 \"role\": \"user\",\n",
    "148.                 \"content\": (\n",
    "149.                     \"\"\n",
    "150.                     + f\"{context}\\n\"\n",
    "151.                     + \"<file_list>\\n\"\n",
    "152.                     + \"\\n\".join(files)\n",
    "153.                     + \"\\n</file_list>\\n\"\n",
    "154.                     + \"<user_instruction>\\n\"\n",
    "155.                     + f\"{input}\\n\"\n",
    "156.                     + \"</user_instruction>\\n\"\n",
    "157.                 ),\n",
    "158.             }\n",
    "159.         ]\n",
    "160.         implementation_plan = create_implementation_plan.invoke(\n",
    "161.             self.client,\n",
    "162.             messages=messages,\n",
    "163.             temperature=1,\n",
    "164.             model=MODEL,\n",
    "165.             stream=False,\n",
    "166.             force=True,\n",
    "167.         )\n",
    "168. \n",
    "169.         if isinstance(implementation_plan, list):\n",
    "170.             implementation_plan = implementation_plan[0]\n",
    "171.         messages = implementation_plan.execute(self.client, self.index, self.codebase)\n",
    "172. \n",
    "173.         files_updated = []\n",
    "174.         files_created = []\n",
    "175.         problems = []\n",
    "176.         for msg in messages:\n",
    "177.             if \"Updated file\" in str(msg):\n",
    "178.                 files_updated.append(msg)\n",
    "179.             elif \"Created file\" in str(msg):\n",
    "180.                 files_created.append(msg)\n",
    "181.             else:\n",
    "182.                 problems.append(msg)\n",
    "183. \n",
    "184.         return f\"\"\"\n",
    "185. I've modified and updated the codebase according to your request in {self.codebase.get_active_branch()} branch. Here's what I've done:\n",
    "186. - New files created: {files_created}\n",
    "187. - Existing files updated: {files_updated}\n",
    "188. - Problems encountered: {problems}\n",
    "189. \n",
    "190. Is there anything else I can help you with?\n",
    "191. \"\"\"\n",
    "192. \n",
    "193.     @action(name=\"SummarizeChanges\", stop=True, decorators=[traceable(run_type=\"tool\")])\n",
    "194.     def summarize_changes(self, msg: str):\n",
    "195.         \"\"\"Summarize the changes made in the previous step.\"\"\"\n",
    "196.         messages = [\n",
    "197.             {\"role\": \"user\", \"content\": f\"{msg}\\n####\\n Summarize the message above\"}\n",
    "198.         ]\n",
    "199.         response = self.client.chat.completions.create(\n",
    "200.             model=MODEL,\n",
    "201.             messages=messages,\n",
    "202.             stream=False,\n",
    "203.             temperature=0.1,\n",
    "204.             token_usage_tracker=TokenUsageTracker(500),\n",
    "205.         )\n",
    "206.         content = \"\"\n",
    "207.         try:\n",
    "208.             content = response.choices[0].message.content\n",
    "209.         except:\n",
    "210.             content = str(response)\n",
    "211.         return content\n",
    "212. \n",
    "213.     def refresh(self):\n",
    "214.         self.codebase.clear_cache()\n",
    "215.         self.index.setup()\n",
    "</snippet autocoder/bot.py>\n",
    "<snippet autocoder/codebase.py start_line=5 end_line=65>\n",
    "5.     def __init__(self, github_api):\n",
    "6.         self.github_api = github_api\n",
    "7.         self.file2code = {}\n",
    "8. \n",
    "9.     def list_files_in_bot_branch(self):\n",
    "10.         content = self.github_api.list_files_in_bot_branch()\n",
    "11.         files = content.split(\"\\n\")[1:]\n",
    "12.         return files\n",
    "13. \n",
    "14.     def clear_cache(self):\n",
    "15.         self.file2code = {}\n",
    "16. \n",
    "17.     def create_pull_request(self, pr_query):\n",
    "18.         return self.github_api.create_pull_request(pr_query)\n",
    "19. \n",
    "20.     def get_active_branch(self):\n",
    "21.         return self.github_api.active_branch\n",
    "22. \n",
    "23.     def get_issues(self):\n",
    "24.         return self.github_api.get_issues()\n",
    "25. \n",
    "26.     def create_branch(self, branch: str):\n",
    "27.         return self.github_api.create_branch(branch)\n",
    "28. \n",
    "29.     def read_file(self, filepath):\n",
    "30.         response = None\n",
    "31. \n",
    "32.         if filepath not in self.file2code:\n",
    "33.             response = self.read_file_wrapper(filepath)\n",
    "34. \n",
    "35.             # TODO: throw an exception if the file is not found\n",
    "36.             if f\"File not found `{filepath}`\" in response:\n",
    "37.                 return None\n",
    "38.             self.file2code[filepath] = response\n",
    "39. \n",
    "40.         response = self.file2code[filepath]\n",
    "41. \n",
    "42.         return response\n",
    "43. \n",
    "44.     def read_file_wrapper(self, filepath):\n",
    "45.         response = self.github_api.read_file(filepath)\n",
    "46. \n",
    "47.         return response\n",
    "48. \n",
    "49.     def read_files(self, files: List[str]) -> List[str]:\n",
    "50.         response = {}\n",
    "51.         for file in files:\n",
    "52.             read_file_response = self.read_file(file)\n",
    "53. \n",
    "54.             if read_file_response:\n",
    "55.                 response[file] = read_file_response\n",
    "56.         return response\n",
    "57. \n",
    "58.     def create_file(self, file_query):\n",
    "59.         return self.github_api.create_file(file_query)\n",
    "60. \n",
    "61.     def update_file(self, content):\n",
    "62.         file_path = content.split(\"\\n\")[0]\n",
    "63.         self.file2code.pop(file_path, None)\n",
    "64. \n",
    "65.         return self.github_api.update_file(content)\n",
    "</snippet autocoder/codebase.py>\n",
    "<snippet autocoder/index.py start_line=26 end_line=66>\n",
    "26. class RepositoryIndex:\n",
    "27.     def __init__(self, github_repository, codebase):\n",
    "28.         self.codebase = codebase\n",
    "29.         self.github_repository = github_repository\n",
    "30. \n",
    "31.         self.llm = OpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.1, max_tokens=256)\n",
    "32.         self.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "33.         self.setup()\n",
    "34. \n",
    "35.     def setup(self):\n",
    "36.         files = self.codebase.list_files_in_bot_branch()\n",
    "37.         self.files = [file for file in files if \".py\" in file]\n",
    "38. \n",
    "39.         # TODO: print messages when loading files\n",
    "40.         self.documents = []\n",
    "41.         for i, file in enumerate(self.files):\n",
    "42.             text = self.codebase.read_file(file)\n",
    "43.             self.documents.append(Document(text=text, metadata={\"file\": file}))\n",
    "44. \n",
    "45.         code_splitter = CodeSplitter(\n",
    "46.             language=\"python\", chunk_lines=40, chunk_lines_overlap=25, max_chars=2000\n",
    "47.         )\n",
    "48.         self.service_context = ServiceContext.from_defaults(\n",
    "49.             llm=self.llm, embed_model=self.embed_model, text_splitter=code_splitter\n",
    "50.         )\n",
    "51.         self.index = VectorStoreIndex.from_documents(\n",
    "52.             self.documents, service_context=self.service_context\n",
    "53.         )\n",
    "54. \n",
    "55.     def _retrieve_with_transform(self, query_bundle: QueryBundle):\n",
    "56.         base_retriever = self.index.as_retriever(\n",
    "57.             similarity_top_k=50, response_mode=\"no_text\"\n",
    "58.         )\n",
    "59. \n",
    "60.         # Use Hypothetical Document Embeddings (HyDE)\n",
    "61.         hyde = HyDEQueryTransform(include_original=True, llm=self.llm)\n",
    "62. \n",
    "63.         transform_retriever = TransformRetriever(base_retriever, hyde)\n",
    "64. \n",
    "65.         retrieved_nodes = transform_retriever.retrieve(query_bundle)\n",
    "66.         return retrieved_nodes\n",
    "</snippet autocoder/index.py>\n",
    "<snippet autocoder/telemetry.py start_line=0 end_line=39>\n",
    "0. import os\n",
    "1. \n",
    "2. from actionweaver.llms import patch\n",
    "3. from langsmith.run_helpers import traceable as _traceable\n",
    "4. \n",
    "5. os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "6. os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "7. project_name = \"autocoder\"\n",
    "8. os.environ[\"LANGCHAIN_PROJECT\"] = project_name  # Optional: \"default\" is used if not set\n",
    "9. \n",
    "10. assert os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "11. \n",
    "12. \n",
    "13. def identity_decorator(func):\n",
    "14.     def wrapper(*args, **kwargs):\n",
    "15.         result = func(*args, **kwargs)\n",
    "16.         return result\n",
    "17. \n",
    "18.     return wrapper\n",
    "19. \n",
    "20. \n",
    "21. def traceable(*args, **kwargs):\n",
    "22.     if os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
    "23.         return _traceable(*args, **kwargs)\n",
    "24.     else:\n",
    "25.         return identity_decorator\n",
    "26. \n",
    "27. \n",
    "28. def trace_client(client):\n",
    "29.     if os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
    "30.         client.chat.completions.create = traceable(name=\"llm_call\", run_type=\"llm\")(\n",
    "31.             client.chat.completions.create\n",
    "32.         )\n",
    "33.         client = patch(client)\n",
    "34.         client.chat.completions.create = traceable(\n",
    "35.             name=\"chat_completion_create\", run_type=\"llm\"\n",
    "36.         )(client.chat.completions.create)\n",
    "37.         return client\n",
    "38.     else:\n",
    "39.         return client\n",
    "</snippet autocoder/telemetry.py>\n",
    "<snippet main.py start_line=0 end_line=66>\n",
    "0. import os\n",
    "1. \n",
    "2. from langchain_community.utilities.github import GitHubAPIWrapper\n",
    "3. \n",
    "4. from autocoder.bot import AutoCoder\n",
    "5. from autocoder.codebase import Codebase\n",
    "6. from autocoder.index import RepositoryIndex\n",
    "7. \n",
    "8. assert os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "9. assert os.environ[\"GITHUB_APP_ID\"]\n",
    "10. assert os.environ[\"GITHUB_APP_PRIVATE_KEY\"]\n",
    "11. \n",
    "12. \n",
    "13. # If use OpenAI API\n",
    "14. assert os.environ[\"OPENAI_API_KEY\"]\n",
    "15. \n",
    "16. \n",
    "17. os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "18. os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "19. \n",
    "20. project_name = \"autocoder\"\n",
    "21. os.environ[\"LANGCHAIN_PROJECT\"] = project_name  # Optional: \"default\" is used if not set\n",
    "22. \n",
    "23. \n",
    "24. def bold_green_string(text):\n",
    "25.     bold_green_text = \"\\033[1;32m\" + text + \"\\033[0m\"\n",
    "26.     return bold_green_text\n",
    "27. \n",
    "28. \n",
    "29. def bold_blue_string(text):\n",
    "30.     bold_blue_text = \"\\033[1;34m\" + text + \"\\033[0m\"\n",
    "31.     return bold_blue_text\n",
    "32. \n",
    "33. \n",
    "34. def stream_string_to_terminal(s, delay=0.1):\n",
    "35.     import time\n",
    "36. \n",
    "37.     for char in s:\n",
    "38.         print(char, end=\"\", flush=True)\n",
    "39.         time.sleep(delay)\n",
    "40.     print()  # for newline after streaming\n",
    "41. \n",
    "42. \n",
    "43. github_repository = \"TengHu/auto_coder\"\n",
    "44. github_api = GitHubAPIWrapper(\n",
    "45.     github_repository=github_repository,\n",
    "46.     github_app_id=os.environ[\"GITHUB_APP_ID\"],\n",
    "47.     github_app_private_key=os.environ[\"GITHUB_APP_PRIVATE_KEY\"],\n",
    "48. )\n",
    "49. codebase = Codebase(github_api)\n",
    "50. index = RepositoryIndex(github_repository, codebase)\n",
    "51. \n",
    "52. autocoder = AutoCoder(index, codebase)\n",
    "53. \n",
    "54. print(\n",
    "55.     bold_green_string(\"Welcome to AutoCoder! Enter your query or type 'exit' to leave\")\n",
    "56. )\n",
    "57. while True:\n",
    "58.     try:\n",
    "59.         user_input = input(bold_green_string(\"User :\"))\n",
    "60.         if user_input.lower() == \"exit\":\n",
    "61.             break\n",
    "62.         res = autocoder(user_input)\n",
    "63.         print(bold_blue_string(\"Assistant: \"))\n",
    "64.         stream_string_to_terminal(res, 0.01)\n",
    "65.     except KeyboardInterrupt:\n",
    "66.         break\n",
    "</snippet main.py>\n",
    "</context>\n",
    "<action_related_to_content>\n",
    "- No detailed instruction required, just replace all occurrences of `github_api` with `langchain_github_api`.\n",
    "</action_related_to_content>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4176e79-8f58-4ad9-a5a5-599b9cb3aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": content\n",
    "    },\n",
    "]\n",
    "blocks = create_blocks.invoke(\n",
    "    auto_coder.client,\n",
    "    messages=messages,\n",
    "    model=os.environ[\"MODEL\"],\n",
    "    temperature=.1,\n",
    "    stream=False,\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8861ffd-c5e7-44c1-b40d-3e24e9492898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def __init__(self, langchain_github_api):\n",
      "        self.langchain_github_api = langchain_github_api\n",
      "        self.file2code = {}\n",
      "\n",
      "    def list_files_in_bot_branch(self):\n",
      "        content = self.langchain_github_api.list_files_in_bot_branch()\n",
      "        files = content.split(\"\\n\")[1:]\n",
      "        return files\n",
      "\n",
      "    def clear_cache(self):\n",
      "        self.file2code = {}\n",
      "\n",
      "    def create_pull_request(self, pr_query):\n",
      "        return self.langchain_github_api.create_pull_request(pr_query)\n",
      "\n",
      "    def get_active_branch(self):\n",
      "        return self.langchain_github_api.active_branch\n",
      "\n",
      "    def get_issues(self):\n",
      "        return self.langchain_github_api.get_issues()\n",
      "\n",
      "    def create_branch(self, branch: str):\n",
      "        return self.langchain_github_api.create_branch(branch)\n",
      "\n",
      "    def read_file(self, filepath):\n",
      "        response = None\n",
      "\n",
      "        if filepath not in self.file2code:\n",
      "            response = self.read_file_wrapper(filepath)\n",
      "\n",
      "            # TODO: throw an exception if the file is not found\n",
      "            if f\"File not found `{filepath}`\" in response:\n",
      "                return None\n",
      "            self.file2code[filepath] = response\n",
      "\n",
      "        response = self.file2code[filepath]\n",
      "\n",
      "        return response\n",
      "\n",
      "    def read_file_wrapper(self, filepath):\n",
      "        response = self.langchain_github_api.read_file(filepath)\n",
      "\n",
      "        return response\n",
      "\n",
      "    def read_files(self, files: List[str]) -> List[str]:\n",
      "        response = {}\n",
      "        for file in files:\n",
      "            read_file_response = self.read_file(file)\n",
      "\n",
      "            if read_file_response:\n",
      "                response[file] = read_file_response\n",
      "        return response\n",
      "\n",
      "    def create_file(self, file_query):\n",
      "        return self.langchain_github_api.create_file(file_query)\n",
      "\n",
      "    def update_file(self, content):\n",
      "        file_path = content.split(\"\\n\")[0]\n",
      "        self.file2code.pop(file_path, None)\n",
      "\n",
      "        return self.langchain_github_api.update_file(content)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (blocks[0].operations[0].new_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8963d7b8-a2ad-4d6c-9773-70045a809719",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"autocoder/codebase.py\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "079ff717-3d7f-4726-83ae-2267b7d5c501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Updated file autocoder/codebase.py'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks[0].operations[0].execute(file_path, auto_coder.client, codebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "961f3960-d50b-4f0e-9008-512128a898f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockOpOnLineIdx(start_line_idx=6, end_line_idx=6, new_code='        self.langchain_github_api = github_api')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks[0].operations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8f6a99f-5245-4d66-86ea-62e391909e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "c, f, l = codebase.map_char_idx_to_line_idx(file_path, 0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a03527a8-4c87-4f8c-85f8-dc6c5977f74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[f:l+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bd4297-a57c-4f19-967d-eef92586f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        # self.client = trace_client(langchain_github_api.AzureOpenAI(\n",
      "        #     azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
      "        #     api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
      "        #     api_version=\"2023-10-01-preview\"\n",
      "        # ))\n",
      "        self.client = trace_client(langchain_github_api.OpenAI())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"        # self.client = trace_client(langchain_github_api.AzureOpenAI(\\n        #     azure_endpoint = os.getenv(\\\"AZURE_OPENAI_ENDPOINT\\\"),\\n        #     api_key=os.getenv(\\\"AZURE_OPENAI_KEY\\\"),\\n        #     api_version=\\\"2023-10-01-preview\\\"\\n        # ))\\n        self.client = trace_client(langchain_github_api.OpenAI())\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222d7ce-af15-4d22-bf62-887207303de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
